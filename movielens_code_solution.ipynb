{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b6f40339-714c-4bb6-8b4e-ce0b31be030a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Problem statment:\n",
    "Please calculate the average ratings, per genre and year.\n",
    "By year we mean the year in which the movies were released.\n",
    "Please consider only movies, which were released after 1989.\n",
    "Please consider the ratings of all persons aged 18-49 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "82b18e12-6975-4647-b1cd-ed938170e09f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Provide path where you have saved your .dat files here by replacing \"your_path\" with actual path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95746b93-74e8-4241-a5cc-b89d67f978a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Provide path where you have saved your .dat files here by replacing \"your_path\" with actual path\n",
    "path=\"your_path\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c02b69f-f11f-4881-84dc-f29934ace143",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, regexp_extract, col, avg\n",
    "\n",
    "try:\n",
    "    # Initialize Spark session and read path\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"MovieLens Average Ratings\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    movies_path = path+\"/movies.dat\"\n",
    "    ratings_path = path+\"/ratings.dat\"\n",
    "    users_path = path+\"/users.dat\"\n",
    "\n",
    "    #Read users data and Filter users aged between 18â€“49 \n",
    "    users_df = spark.read.text(users_path)\n",
    "\n",
    "    users_df = users_df.select(\n",
    "        split(users_df.value, \"::\").getItem(0).alias(\"UserID\"),\n",
    "        split(users_df.value, \"::\").getItem(2).alias(\"Age\"))\n",
    "\n",
    "    users_df = users_df.filter((users_df.Age>=18) & (users_df.Age<=49))\n",
    "\n",
    "    # Read movies data\n",
    "    movies_df = spark.read.text(movies_path)\n",
    "    movies_df = movies_df.select(\n",
    "        split(movies_df.value, \"::\").getItem(0).alias(\"MovieID\"),\n",
    "        split(movies_df.value, \"::\").getItem(1).alias(\"Title\"),\n",
    "        split(movies_df.value, \"::\").getItem(2).alias(\"Genres\")\n",
    "    )\n",
    "\n",
    "    ##DataQuality Checks applied on movies dataframe as the Readme file mentions that this file may have inconsistent and duplicate data##\n",
    "    # 1. Filter out movies without valid year or null genres\n",
    "    # 2. Dropping duplicate MovieId\n",
    "    cleaned_movies_df = movies_df.filter(\n",
    "        (col(\"Title\").rlike(r\"\\(\\d{4}\\)\")) &\n",
    "        (~col(\"Genres\").isNull()) &\n",
    "        (col(\"Genres\") != \"\")\n",
    "    )\n",
    "    cleaned_movies_df = cleaned_movies_df.dropDuplicates([\"MovieID\"])\n",
    "\n",
    "    # Extract year from title\n",
    "    year_movies_df = cleaned_movies_df.withColumn(\"Year\", \\\n",
    "                        regexp_extract(col(\"Title\"), r\"\\((\\d{4})\\)\", 1) \\\n",
    "                        .cast(\"int\")) \\\n",
    "                        .filter(col(\"Year\") > 1989)\n",
    "\n",
    "    # Explode genres\n",
    "    final_movies_df = year_movies_df.withColumn(\"Genre\", \\\n",
    "            explode(split(col(\"Genres\"), \"\\\\|\"))) \\\n",
    "            .drop(\"Genres\")\n",
    "\n",
    "    # Read ratings data\n",
    "    ratings_df = spark.read.text(ratings_path)\n",
    "    ratings_df = ratings_df.select(\n",
    "        split(ratings_df.value, \"::\").getItem(0).alias(\"UserID\"),\n",
    "        split(ratings_df.value, \"::\").getItem(1).alias(\"MovieID\"),\n",
    "        split(ratings_df.value, \"::\").getItem(2).alias(\"Rating\")\n",
    "    )\n",
    "\n",
    "    #Joining the three dataframes\n",
    "    joined_df = ratings_df.join(users_df, \"UserId\", \"inner\") \\\n",
    "                .join(final_movies_df, \"MovieID\", \"inner\")\n",
    "\n",
    "    # Group by Year and Genre, compute average rating\n",
    "    result_df = joined_df.groupBy(\"Year\", \"Genre\") \\\n",
    "                .agg(avg(\"Rating\").alias(\"AvgRating\")) \\\n",
    "                .orderBy(\"Year\", \"Genre\")\n",
    "\n",
    "    # Saving the result in two ways:\n",
    "    # 1. Writing all the data to a single csv file for reading purpose only\n",
    "    result_df.coalesce(1).write \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .csv(path+\"output/avg_ratings\")\n",
    "\n",
    "    # 2. Assuming for big data coming, will keep this Partitioning strategy on Year and Genre\n",
    "    result_df.write \\\n",
    "            .option(\"header\", \"true\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .partitionBy(\"Year\", \"Genre\") \\\n",
    "            .csv(path+\"output/avg_ratings_partitioned\")\n",
    "\n",
    "except Exception as e:\n",
    "        print(f\"An error occurred during processing: {e}\", exc_info=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8ee4b152-986c-480c-a922-c3e7fed2ae55",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "ed587cfa-a63c-4037-af9d-67d0409c8e9b",
     "origId": 3039764707764288,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "movielens_code_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
